import numpy as np
#import numba 
#from numba import prange

from psf_funs import *
from optim_modules import *
from optim_modules import select_optimizer

import warnings


def select_psf_1d(psf_name,psf_param):

    if psf_name == 'sinc':
        ftype = 'Sinc Intensity Function'
        psf_fn = sincsq_1d
        psf_param2 = psf_param
    elif psf_name == 'sinc_pi':
        ftype = 'Sinc Intensity Function with Pi Scaling'
        psf_fn = sincsq_pi_1d
        psf_param2 = psf_param
    elif psf_name == 'exp':
        ftype = "Esponential/Gaussian Intensity Function"
        psf_fn = expsq_1d
        psf_param2 = psf_param
    elif psf_name == 'polynomial':
        ftype = 'Polynomial Intensity Function'
        psf_fn = polynomial_1d
        psf_param2 = {}
        psf_param2['coeff'] = np.array(psf_param).reshape(1,-1)
        psf_param2['idx'] = np.arange(1,len(psf_param)+1).reshape(-1,1)
        #psf_param2['x_mat'] = creat_x_mat(x,len(psf_param))
    elif psf_name == 'fourier':
        ftype = 'Fourier Intensity Funtion'
        psf_fn = fourier_1d
        psf_param2 = psf_param
    else:
        psf_fn = None

    return psf_fn, psf_param2, ftype


def default_init_1d(n_components, variables_to_opt, psf_scale=1.0):
    init = np.zeros((n_components,3))
    if variables_to_opt[0]: #delay
        init[:,0] = np.pi * 0.1 * np.random.randn(n_components)
    else:
        init[:,0] = 0
        warnings.warn('Delay for all components set to 0. Consider Manually setting initialization.')
    
    if variables_to_opt[1]: #amplitude
        init[:,1] = (1 - 0.1 * (np.random.rand(n_components) - 0.5))/n_components
    else:
        init[:,1] = 1.0/n_components

    if variables_to_opt[2]: #scale
        init[:,2] = psf_scale - 0.1 * (np.random.rand(n_components) - 0.5)
    else:
        init[:,2] = psf_scale

    return init


class AIRYSEARCH_1D:
    def __init__(self,
                 x,
                 n_components,
                 psf='sinc',
                 psf_param=None,
                 variables_to_opt = [1, 0, 0], #[Theta, Amplitude, Scale]
                 psf_scale = 1,
                 init = None,
                 seed = None,
                 optimizer = 'adam',
                 optimizer_parameter = None,
                 max_iterations=100000,
                 init_learning_rate=0.1
                 ):

        self.n_components = n_components
        self.x = x
        self.variables_to_opt = variables_to_opt
        self.max_it = max_iterations
        self.init_learning_rate = init_learning_rate
        self.signal_size = self.x.shape
        self.axis_to_reduce = 1

        self.psf_fn, self.psf_param, self.ftype = select_psf_1d(psf,psf_param)
        
        if seed is not None:
            state = np.random.get_state()
            np.random.seed(seed)
        if init is None:
            self.init = default_init_1d(self.n_components, self.variables_to_opt, psf_scale)
        else:
            self.init = init
        
        if seed is not None:
            np.random.set_state(state)

        self.est = self.init.copy()
        self.loss = []
        self.estimates = []

        self.optimizer = select_optimizer(optimizer_name=optimizer, 
                                          est = self.est,
                                          optimizer_parameter=optimizer_parameter)
        return

    
    def measurement(self):
        y = np.zeros(self.x.shape)
        for i in range(n_components):
            z, _ = self.psf_fn(self.x,param = self.est[i,:],
                    psf_param=self.psf_param,compute_grad=False)
            y += z
        return y

    def compute_grad(self, observed_intensity):
        #compute gradients and estimated intensity
        est_intensity = np.zeros(self.signal_size)
        expnaded_grads = []
        grads = np.zeros(self.est.shape)
        for i in range(self.n_components):
            z, grad = self.psf_fn(self.x, param2opt= self.est[i,:],
                                psf_param=self.psf_param,
                                compute_grad=True,
                                variables_to_opt=self.variables_to_opt)
            est_intensity += z
            expnaded_grads.append(grad)
        
        difference = est_intensity-observed_intensity
        self.loss.append(np.mean(difference**2))

        for i in range(self.n_components):
            grads[i,:] = np.mean(2*difference*expnaded_grads[i],axis=self.axis_to_reduce)
            
        return grads

    def fit(self, observed_intensity, verbose=True):
        self.loss = []
        self.estimates = []
        
        self.est = self.init.copy()
        self.estimates.append(self.est.copy())
        if verbose == True:
            print("**Starting State:**")
            self.print_params()
        learning_rate = self.init_learning_rate
        for it_no in range(self.max_it):
            if (it_no+1)%3000 == 0:
                learning_rate = learning_rate/10; #
            #learning_rate = self.init_learning_rate * (1 - it_no/self.max_it)
            #compute gradients
            grad = self.compute_grad(observed_intensity)        
            #Compute New Estimate
            self.optimizer.optimize(self.est, grad, learning_rate)
            self.estimates.append(self.est.copy())
        
        self.estimates = np.array(self.estimates)
        
        if verbose == True:
            print("**Optimized State:**")
            self.print_params()
        return

    def print_params(self):
        print('**Estimator State**')
        print('Total Number of Components: ', self.n_components)
        print('Function Type: ', self.ftype)
        for i in range(self.n_components):
            print('Coponent ', i+1, ': Amplitude = ', self.est[i,1], 
                    ', Delay = ', self.est[i,0], ', Scale = ', self.est[i,2])

        return
        
class AIRYSEARCH_1D_2PTS(AIRYSEARCH_1D):
    def __init__(self,
                 x,
                 psf='sinc',
                 psf_param=None,
                 variables_to_opt = [1, 0, 0], #[Theta, Amplitude, Scale]
                 psf_scale = 1,
                 init = None,
                 seed = None,
                 optimizer = 'adam',
                 optimizer_parameter = None,
                 max_iterations=100000,
                 init_learning_rate=0.1):
        
        self.n_components = 1
        self.x = x
        self.variables_to_opt = variables_to_opt
        self.max_it = max_iterations
        self.init_learning_rate = init_learning_rate
        self.signal_size = self.x.shape
        self.axis_to_reduce = 1
        self.psf_fn, self.psf_param, self.ftype = select_psf_1d(psf,psf_param)
        
        
        if seed is not None:
            state = np.random.get_state()
            np.random.seed(seed)
            
        if init is None:
            self.init = default_init_1d(self.n_components, self.variables_to_opt, psf_scale)
            self.init[:,1]=0.5
        else:
            self.init = init
        
        if seed is not None:
            np.random.set_state(state)
            
        self.est = self.init.copy()
        self.loss = []
        self.estimates = []
        
        #select Optimizer    
        self.optimizer = select_optimizer(optimizer_name=optimizer, 
                                          est = self.est,
                                          optimizer_parameter=optimizer_parameter)
                                          
                                          
        return
        
    def measurement(self):
        param_1 = self.est.copy()
        z1, _ = self.psf_fn(self.x,param2opt = param_1[0,:],
                    psf_param=self.psf_param,compute_grad=False)
        
        param_1 = self.est.copy()
        param_1[0,0] = -param_1[0,0] #theta parameter is negative for this one
        z2, _ = self.psf_fn(self.x,param2opt = param_1[0,:],
                    psf_param=self.psf_param,compute_grad=False)
        y = z1 + z2
        return y
        
    def compute_grad(self, observed_intensity):
        #compute gradients and estimated intensity
        expanded_grads = []
        grads = np.zeros(self.est.shape)
        
        param_1 = self.est.copy()
        #print('PARAMS:')
        #print(param_1)
        z1, grad1 = self.psf_fn(self.x, param2opt= param_1[0,:],
                                psf_param=self.psf_param,
                                compute_grad=True,
                                variables_to_opt=self.variables_to_opt)
        param_2 = self.est.copy()
        param_2[0,0] = -param_2[0,0]
        #print(param_2)
        z2, grad2 = self.psf_fn(self.x, param2opt= param_2[0,:],
                                psf_param=self.psf_param,
                                compute_grad=True,
                                variables_to_opt=self.variables_to_opt)
                                
        expanded_grads.append(grad1-grad2)
        est_intensity = z1+z2
        
        difference = est_intensity-observed_intensity
        self.loss.append(np.mean(difference**2))

        for i in range(self.n_components):
            grads[i,:] = np.mean(2*difference*expanded_grads[i],axis=self.axis_to_reduce)
        
        return grads
                

def default_init_2d(n_components, variables_to_opt, psf_scale=1.0):
    init = np.zeros((n_components,4))
    if variables_to_opt[0]:
        init[:,0:2] = np.pi * 0.5 * np.random.randn(n_components,2)
    else:
        init[:,0:2] = 0
        warnings.warn('Delay for all components set to 0. Consider Manually setting initialization.')
    
    if variables_to_opt[1]:
        init[:,2] = (1 - 0.1 * (np.random.rand(n_components) - 0.5))/n_components
    else:
        init[:,2] = 1.0

    if variables_to_opt[2]:
        init[:,3] = psf_scale - 0.1 * (np.random.rand(n_components) - 0.5)
    else:
        init[:,3] = psf_scale

    return init


class AIRYSEARCH_2D(AIRYSEARCH_1D):
    def __init__(self,
                 x,y,
                 n_components,
                 psf='sinc',
                 psf_param=None,
                 variables_to_opt = [1, 0, 0],
                 psf_scale = 1,
                 init = None,
                 seed = None,
                 optimizer = 'adam',
                 optimizer_parameter = None
                 ):

        self.n_components = n_components
        X,Y = np.meshgrid(x,y)
        self.x = (X,Y)
        self.variables_to_opt = variables_to_opt
        self.max_it = 100000
        self.init_learning_rate = 0.1
        self.signal_size = self.x[0].shape
        self.axis_to_reduce = (1,2)

        self.psf_fn, self.psf_param, self.ftype = select_psf_2d(psf,psf_param)
        
        if seed is not None:
            state = np.random.get_state()
            np.random.seed(seed)
        if init is None:
            self.init = default_init_2d(self.n_components, self.variables_to_opt, psf_scale)
        else:
            self.init = init
        if seed is not None:
            np.random.set_state(state)

        self.est = self.init.copy()
        self.loss = []
 
        self.optimizer = select_optimizer(optimizer_name=optimizer, 
                                          est = self.est,
                                          optimizer_parameter=optimizer_parameter)
